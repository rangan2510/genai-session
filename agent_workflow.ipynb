{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6eea0c6",
   "metadata": {},
   "source": [
    "# Topic :  Open API and AI Agents Code-Along\n",
    "\n",
    "Adopted from [Colab Notebook](https://colab.research.google.com/drive/1xYq4wr4dkmvOuq0Ljwt1W9fIxfR50ekq)\n",
    "\n",
    "This notebook has 4 main sections:\n",
    "* Using AgentWorkflow\n",
    "* Building a multi-agent system with AgentWorkflow\n",
    "* Building agentic Workflows\n",
    "* Building a multi-agent system with Workflows\n",
    "\n",
    "In LlamaIndex, `Workflow`s are the building blocks of putting together an agent or a multi-agent system. To get you up and running quickly, we have a pre-built Workflow called the `AgentWorkflow` that handles a lot of common agent use cases and allows you to focus on the specifics of your implementation. Later, we drop down one level to Workflows themselves and show how to build one from scratch.\n",
    "\n",
    "To start off, we'll create an AgentWorkflow with a single agent, and use that to cover the basic functionality of the `AgentWorkflow` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30bbf1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: sk-proj-Cq\n",
      "TAVILY_API_KEY: tvly-dev-x\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "print(\"OPENAI_API_KEY:\", OPENAI_API_KEY[:10])\n",
    "print(\"TAVILY_API_KEY:\", TAVILY_API_KEY[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70140117",
   "metadata": {},
   "source": [
    "With our API key in hand, instantiating an LLM is one line..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b870fedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I donâ€™t have access to past interactions in this conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm2 = OpenAI(model=\"o4-mini-2025-04-16\", api_key=OPENAI_API_KEY, max_tokens=100000)\n",
    "llm = OpenAI(model=\"gpt-4.1-mini-2025-04-14\", api_key=OPENAI_API_KEY)\n",
    "response = llm.complete(\"What did I ask you before?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c68e17",
   "metadata": {},
   "source": [
    "## Building a tool\n",
    "\n",
    "A core feature of agents is that they can use tools to find out more about the state of the world, or take action in response to instructions, without you needing to be explicit about how and when they do that.\n",
    "\n",
    "In this example, we'll give our agent the ability to search the web for information by creating a tool that does that. We'll use a service called Tavily, which is specifically designed to provide this kind of tool to agents. You can get a free API key from [Tavily](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a106e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient\n",
    "\n",
    "# note the type annotations for the incoming query and the return string\n",
    "# Performs a basic web search using Tavily and returns the result as a string.\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))\n",
    "\n",
    "# Performs an advanced web search with more depth and results, returns the result as a string.\n",
    "async def advanced_search_web(query: str) -> dict:\n",
    "    \"\"\"Useful for performing in-depth web searches web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    return str(\n",
    "        await client.search(\n",
    "            query=query,\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=20,\n",
    "            include_answer=\"advanced\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Crawls a website starting from the base_url, following links up to a certain depth and breadth.\n",
    "async def crawl_web(base_url: str) -> str:\n",
    "    \"\"\"Useful for using the looking up URL for citations\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    results = await client.crawl(\n",
    "        url=base_url,\n",
    "        max_depth=3,\n",
    "        limit=20,\n",
    "        max_breadth=20,\n",
    "        extract_depth=\"advanced\",\n",
    "        select_domains=[\"\"],\n",
    "        allow_external=True,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Extracts text content from a given URL using Tavily's extraction capabilities.\n",
    "async def extract_text_from_url(url: str) -> dict:\n",
    "    \"\"\"Extract text from a URL.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "    response = await client.extract(urls=[url], extract_depth=\"advanced\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "await advanced_search_web(\"Types of coffee beans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9cd1d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_search import search_local_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da4565",
   "metadata": {},
   "source": [
    "## Instantiating an AgentWorkflow\n",
    "\n",
    "With the tool and and LLM defined, we can create an `AgentWorkflow` that uses the tool. AgentWorkflow has a special helper method for creating a single agent from a set of tools, so we'll use that.\n",
    "\n",
    "We give it a system prompt that defines what the agent does. It's a good idea to tell the agent what kinds of things its tools will allow it to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_web],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that answers questions. If you don't know the answer, you can search the web for information. Format your response in markdown, prefer tables, and include citations for any information you find online.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d0cf3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [search_local_corpus],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that answers questions from the locally stored oncology handbook. If you don't know the answer, tell it clearly. Format your response in markdown, prefer tables, and include citations for any information you find online.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b40d8",
   "metadata": {},
   "source": [
    "## Running the Agent\n",
    "\n",
    "Now that our agent is created, we can run it! An AgentWorkflow expects to start with a question or prompt in the `user_msg`, which it passes to the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ed6273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The relevant genes commonly associated with colorectal cancer include:\n",
       "\n",
       "| Gene       | Role/Significance                                                                                  |\n",
       "|------------|--------------------------------------------------------------------------------------------------|\n",
       "| APC        | Tumor suppressor gene mutated in about 70-80% of colorectal cancers; involved in Wnt signaling pathway regulation. |\n",
       "| KRAS       | Oncogene mutated in colorectal cancer; mutations lead to uncontrolled cell growth.                |\n",
       "| TP53       | Tumor suppressor gene frequently mutated in colorectal cancer; involved in apoptosis and DNA repair. |\n",
       "| Mismatch Repair Genes (MLH1, MSH2, MSH6, PMS2) | Mutations cause microsatellite instability and are linked to hereditary non-polyposis colorectal cancer (Lynch syndrome). |\n",
       "| BRAF       | Oncogene; mutations can be involved in colorectal cancer progression.                             |\n",
       "| PTEN       | Tumor suppressor gene; loss leads to activation of growth factor signaling pathways.              |\n",
       "\n",
       "Additional notes:\n",
       "- APC mutations are often the initiating event in colorectal tumorigenesis.\n",
       "- KRAS mutations occur in a significant subset and predict poor prognosis and resistance to certain therapies (e.g., anti-EGFR therapy).\n",
       "- Microsatellite instability due to mismatch repair gene defects is a hallmark of a subset of colorectal cancers and is associated with better prognosis but lack of benefit from fluorouracil adjuvant therapy.\n",
       "- TP53 mutations contribute to tumor progression.\n",
       "- BRAF mutations are also considered in molecular profiling.\n",
       "- Genetic predisposition syndromes such as Familial Adenomatous Polyposis (FAP) and Lynch syndrome involve mutations in APC and mismatch repair genes respectively.\n",
       "\n",
       "References:\n",
       "- The information is based on the oncology handbook chapters on colorectal cancer genetics and molecular biology.\n",
       "- [Adam et al., The Oncosurgery Approach to Managing Liver Metastases from Colorectal Cancer, Oncologist 2013]\n",
       "- [General oncology genetics chapters on tumor suppressor genes and oncogenes]\n",
       "\n",
       "If you need detailed information on any specific gene or its clinical implications, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "response = await workflow.run(user_msg=\"What are the relevant genes for colorectal cancer?\")\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02820a87",
   "metadata": {},
   "source": [
    "## Maintaining State\n",
    "\n",
    "By default, the `AgentWorkflow` is stateless between runs. This means that the agent will not have any memory of previous runs.\n",
    "\n",
    "To maintain state, we need to keep track of the previous state. In LlamaIndex, Workflows have a `Context` that can be used to maintain state within and between runs. Since the `AgentWorkflow` is just a pre-built `Workflow`, we can also use it now.\n",
    "\n",
    "To maintain state between runs, we'll create a new `Context` called `ctx`. We pass in our `workflow` to properly configure this Context object for the workflow that will use it.\n",
    "\n",
    "With our configured Context, we can pass it to our first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94212ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# configure a context to work with our workflow\n",
    "ctx = Context(workflow)\n",
    "\n",
    "response = await workflow.run(\n",
    "    user_msg=\"My name is Rangan, nice to meet you!\", ctx=ctx # give the configured context to the workflow\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82819111",
   "metadata": {},
   "source": [
    "Now we can pass the same context to a second run, and it will remember what happened before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the workflow again with the same context\n",
    "response = await workflow.run(user_msg=\"What is my name?\", ctx=ctx)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327c7e7",
   "metadata": {},
   "source": [
    "You can save and load the agent's context to maintain memory over the longer term. By serializing the `Context` object to disk and restoring it later, the agent can remember previous interactions even after the notebook or process is restarted. This enables persistent memory across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
    "import json\n",
    "\n",
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "with open(\"ctx_dict.json\", \"w\") as f:\n",
    "    json.dump(ctx_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ctx_dict.json\", \"r\") as f:\n",
    "    ctx_dict = json.load(f)\n",
    "restored_ctx = Context.from_dict(\n",
    "    workflow, ctx_dict, serializer=JsonSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdadc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = await workflow.run(user_msg=\"What's my name?\", ctx=restored_ctx)\n",
    "print(str(response3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c336833",
   "metadata": {},
   "source": [
    "## Accessing Context from within tools\n",
    "\n",
    "Tools can also be defined that have access to the workflow context. This means you can set and retrieve variables from the context and use them in the tool or between tools.\n",
    "\n",
    "`AgentWorkflow` uses a context variable called `state` that gets passed to every agent. You can rely on information in `state` being available without explicitly having to pass it in.\n",
    "\n",
    "**Note:** To access the `Context`, the Context parameter should be the first parameter of the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a818227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# we've set the first parameter to be of type Context\n",
    "async def set_name(ctx: Context, name: str) -> str:\n",
    "    state = await ctx.get(\"state\")\n",
    "    state[\"name\"] = name\n",
    "    await ctx.set(\"state\", state)\n",
    "    return f\"Name set to {name}\"\n",
    "\n",
    "\n",
    "stateful_workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [set_name],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
    "    initial_state={\"name\": \"unset\"},\n",
    ")\n",
    "\n",
    "stateful_workflow_context = Context(stateful_workflow)\n",
    "\n",
    "# this workflow will save data to the `state` variable in the context\n",
    "response = await stateful_workflow.run(user_msg=\"My name is Rangan\", ctx=stateful_workflow_context)\n",
    "print(str(response))\n",
    "\n",
    "# we can retrieve the value of `state` from the context directly\n",
    "state = await stateful_workflow_context.get(\"state\")\n",
    "print(\"Name as stored in state: \",state[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc92b8",
   "metadata": {},
   "source": [
    "## Streaming output and events\n",
    "\n",
    "In addition to returning a single string when the workflow completes, the `AgentWorkflow` also supports streaming. Since the `AgentWorkflow` is a `Workflow`, it can be streamed like any other `Workflow`. This works by using the handler that is returned from the workflow. The stream returns a variety of event types as the workflow executes, and you can select which ones to handle.\n",
    "\n",
    "* If you want to stream the LLM output, you can use the `AgentStream` events, which contain a `delta` of the new output each time\n",
    "* `AgentInput` events will tell you which agent is running (our current workflow just has one agent)\n",
    "* `AgentOutput` events will tell you what the agents returned, including which tools they called\n",
    "* `ToolCall` and `ToolCallResults` will track tools as they are called and their outputs\n",
    "\n",
    "In this example we're only handling the `AgentStream` events, but you can see in the commented-out code how we'd handle the other kinds. You can tell our handling is working because the output will appear in chunks as you run the cell, rather than appearing all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = workflow.run(user_msg=\"What is the weather in Saskatoon? Give detailed results in the form of a table\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)\n",
    "        # print(event.response)  # the current full response\n",
    "        # print(event.raw)  # the raw llm api response\n",
    "        # print(event.current_agent_name)  # the current agent name\n",
    "    # elif isinstance(event, AgentInput):\n",
    "    #    print(event.input)  # the current input messages\n",
    "    #    print(event.current_agent_name)  # the current agent name\n",
    "    # elif isinstance(event, AgentOutput):\n",
    "    #    print(event.response)  # the current full response\n",
    "    #    print(event.tool_calls)  # the selected tool calls, if any\n",
    "    #    print(event.raw)  # the raw llm api response\n",
    "    # elif isinstance(event, ToolCallResult):\n",
    "    #    print(event.tool_name)  # the tool name\n",
    "    #    print(event.tool_kwargs)  # the tool kwargs\n",
    "    #    print(event.tool_output)  # the tool output\n",
    "    # elif isinstance(event, ToolCall):\n",
    "    #     print(event.tool_name)  # the tool name\n",
    "    #     print(event.tool_kwargs)  # the tool kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745ee59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = workflow.run(user_msg=\"What is the weather in Saskatoon?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentInput):\n",
    "       print(\"Agent input: \", event.input)  # the current input messages\n",
    "       print(\"Agent name:\", event.current_agent_name)  # the current agent name\n",
    "    elif isinstance(event, AgentOutput):\n",
    "       print(\"Agent output: \", event.response)  # the current full response\n",
    "       print(\"Tool calls made: \", event.tool_calls)  # the selected tool calls, if any\n",
    "       print(\"Raw LLM response: \", event.raw)  # the raw llm api response\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "       print(\"Tool called: \", event.tool_name)  # the tool name\n",
    "       print(\"Arguments to the tool: \", event.tool_kwargs)  # the tool kwargs\n",
    "       print(\"Tool output: \", event.tool_output)  # the tool output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b776a9",
   "metadata": {},
   "source": [
    "# Multi-agent System with AgentWorkflow\n",
    "\n",
    "Our system will have three agents:\n",
    "\n",
    "1. A `ResearchAgent` that will search the web for information on the given topic.\n",
    "2. A `WriteAgent` that will write the report using the information found by the `ResearchAgent`.\n",
    "3. A `ReviewAgent` that will review the report and provide feedback.\n",
    "\n",
    "We will use the `AgentWorkflow` class to create a multi-agent system that will execute these agents in order.\n",
    "\n",
    "There are a lot of ways we could go about building a system to perform this task. In this example, we will use a few tools to help with the research and writing processes.\n",
    "\n",
    "1. A `web_search` tool to search the web for information on the given topic.\n",
    "2. A `record_notes` tool which will save research found on the web to the state so that the other tools can use it.\n",
    "3. A `write_report` tool to write the report using the information found by the `ResearchAgent`\n",
    "4. A `review_report` tool to review the report and provide feedback.\n",
    "\n",
    "Utilizing the `Context` class, we can pass state between agents, and each agent will have access to the current state of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))\n",
    "\n",
    "async def record_notes(ctx: Context, notes: str, notes_title: str) -> str:\n",
    "    \"\"\"Useful for recording notes on a given topic.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    if \"research_notes\" not in current_state:\n",
    "        current_state[\"research_notes\"] = {}\n",
    "    current_state[\"research_notes\"][notes_title] = notes\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Notes recorded.\"\n",
    "\n",
    "async def write_report(ctx: Context, report_content: str) -> str:\n",
    "    \"\"\"Useful for writing a report on a given topic.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"report_content\"] = report_content\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report written.\"\n",
    "\n",
    "async def review_report(ctx: Context, review: str) -> str:\n",
    "    \"\"\"Useful for reviewing a report and providing feedback.\"\"\"\n",
    "    current_state = await ctx.get(\"state\")\n",
    "    current_state[\"review\"] = review\n",
    "    await ctx.set(\"state\", current_state)\n",
    "    return \"Report reviewed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c686b97",
   "metadata": {},
   "source": [
    "With our tools defined, we can now create our agents.\n",
    "\n",
    "If the LLM you are using supports tool calling (most big models do), you can use the `FunctionAgent` class, which is a little more efficient. Otherwise, you can use the `ReActAgent` class.\n",
    "\n",
    "Here, the name and description of each agent is used so that the system knows what each agent is responsible for and when to hand off control to the next agent.\n",
    "\n",
    "Like our single-agent system did before, each agent takes a `system_prompt` that tells it what it should do, and suggests how to work with the other agents.\n",
    "\n",
    "You can also optionally help your multi-agent system constrain itself by listing which other agents an agent can talk to using `can_handoff_to` (otherwise it will just try to figure this out on its own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f778576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "\n",
    "research_agent = FunctionAgent(\n",
    "    name=\"ResearchAgent\",\n",
    "    description=\"Useful for searching the web for information on a given topic and recording notes on the topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ResearchAgent that can search the web for information on a given topic and record notes on the topic. \"\n",
    "        \"Once notes are recorded and you are satisfied, you should hand off control to the WriteAgent to write a report on the topic.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[advanced_search_web, record_notes],\n",
    "    can_handoff_to=[\"WriteAgent\"],\n",
    ")\n",
    "\n",
    "write_agent = FunctionAgent(\n",
    "    name=\"WriteAgent\",\n",
    "    description=\"Useful for writing a report on a given topic.\",\n",
    "    system_prompt=(\n",
    "        \"You are the WriteAgent that can write a report on a given topic. \"\n",
    "        \"Your report should be in a markdown format. The content should be grounded in the research notes. Give citations for any information you find online. Provide URLs.\"\n",
    "        \"Once the report is written, you should get feedback at least once from the ReviewAgent.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[write_report],\n",
    "    can_handoff_to=[\"ReviewAgent\", \"ResearchAgent\"],\n",
    ")\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    name=\"ReviewAgent\",\n",
    "    description=\"Useful for reviewing a report and providing feedback.\",\n",
    "    system_prompt=(\n",
    "        \"You are the ReviewAgent that can review a report and provide feedback. \"\n",
    "        \"Your feedback should either approve the current report or request changes for the WriteAgent to implement.\"\n",
    "    ),\n",
    "    llm=llm,\n",
    "    tools=[review_report],\n",
    "    can_handoff_to=[\"ResearchAgent\",\"WriteAgent\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b6211",
   "metadata": {},
   "source": [
    "## Running the Workflow\n",
    "\n",
    "With our agents defined, we can create our `AgentWorkflow` and run it. Since this is a multi-agent system, instantiating it looks a little different. Instead of using the `from_tools_or_functions` helper, we instantiate it directly. It takes these arguments:\n",
    "* an array of agents\n",
    "* the name of the agent that it should start with (this will receive the initial `user_msg`)\n",
    "* an `initial_state` which populates the context variable `state` we mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[research_agent, write_agent, review_agent],\n",
    "    root_agent=research_agent.name,\n",
    "    initial_state={\n",
    "        \"research_notes\": {},\n",
    "        \"report_content\": \"Not done.\",\n",
    "        \"review\": \"Review required.\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a069976",
   "metadata": {},
   "source": [
    "As the workflow is running, we will stream the events to get an idea of what is happening under the hood. We've added some decoration to the output to make this a little easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb6cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "handler = agent_workflow.run(\n",
    "    user_msg=\"\"\"Analyze the theme of absurdity in Albert Camus's The Stranger. Summarize how Meursault's actions embody existentialist ideas, then connect them to Sartre's notion of freedom and responsibility. Put in multiple illustrative quotes. Add a references section in the end.\"\"\", \n",
    ")\n",
    "\n",
    "current_agent = None\n",
    "current_tool_calls = \"\"\n",
    "async for event in handler.stream_events():\n",
    "    if (\n",
    "        hasattr(event, \"current_agent_name\")\n",
    "        and event.current_agent_name != current_agent\n",
    "    ):\n",
    "        current_agent = event.current_agent_name\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ðŸ¤– Agent: {current_agent}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "    elif isinstance(event, AgentOutput):\n",
    "        if event.response.content:\n",
    "            print(\"ðŸ“¤ Output:\", event.response.content)\n",
    "        if event.tool_calls:\n",
    "            print(\n",
    "                \"ðŸ› ï¸  Planning to use tools:\",\n",
    "                [call.tool_name for call in event.tool_calls],\n",
    "            )\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "        print(f\"ðŸ”§ Tool Result ({event.tool_name}):\")\n",
    "        print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "        print(f\"  Output: {event.tool_output}\")\n",
    "    elif isinstance(event, ToolCall):\n",
    "        print(f\"ðŸ”¨ Calling Tool: {event.tool_name}\")\n",
    "        print(f\"  With arguments: {event.tool_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6934d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "state = await handler.ctx.get(\"state\")\n",
    "display(Markdown(state[\"report_content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e496c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(state[\"review\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b005c2",
   "metadata": {},
   "source": [
    "# Building agentic workflows from scratch\n",
    "\n",
    "As mentioned, `AgentWorkflow` is a pre-built `Workflow`, so you've already encountered many of the concepts you need to build a workflow from scratch.\n",
    "\n",
    "You can easily build your own workflows directly to create multi-agent systems that are customized to your needs. There's a number of reasons you might want to branch out from AgentWorkflow to rolling your own workflow:\n",
    "\n",
    "* **Fine-grained flow control**: Workflows allow looping, branching, parallel calls and map-reduce behavior. Workflows allow you to precisely specify how data gets passed around your system.\n",
    "\n",
    "* **Structured inputs and outputs**: If you want your inputs and outputs to be richer than simple strings, or to kick off with more than just a `user_msg`, workflows allow you to do that.\n",
    "\n",
    "* **Multimodality**: LlamaIndex can handle more than just text! You can handle images, audio and video as well.\n",
    "\n",
    "* **Query planning**: a custom workflow can execute complex planning of how to run based on its inputs, rather than immediately jumping into a team.\n",
    "\n",
    "* **Reflection**: A powerful technique for agents is their ability to examine their own output and decide whether it's sufficient, or if they need to try again. Using looping, you can implement this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332632d",
   "metadata": {},
   "source": [
    "## Creating a new workflow\n",
    "\n",
    "Under the hood, Workflows are regular Python classes. They are defined as a series of `steps`, each of which receives certain classes of events and emits certain classes of events.\n",
    "\n",
    "Here's the most basic form of a workflow, with a single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd679516",
   "metadata": {},
   "source": [
    "This new `MyWorkflow` class:\n",
    "* Uses the `@step` decorator to declare a function to be a step\n",
    "* Has a single step called `my_step` which accepts a `StartEvent`. `StartEvent` is a special event which is always generated when a workflow first runs.\n",
    "* `my_step` returns a `StopEvent`, which is another special event. When a `StopEvent` is emitted the workflow returns it and stops running.\n",
    "\n",
    "You instantiate it and run it just like you ran the the `AgentWorkflow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await workflow.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfee84",
   "metadata": {},
   "source": [
    "## Type annotations for steps\n",
    "\n",
    "The type annotations (e.g. `ev: StartEvent` and `-> StopEvent`) are essential to the way Workflows work. The expected types determine what event types will trigger a step. Tools like the visualizer (see later) also rely on these annotations to determine what types are generated and therefore where control flow goes next.\n",
    "\n",
    "## Aside: running a workflow in regular python\n",
    "\n",
    "Workflows are async by default, so you use `await` to get the result of the `run` command. This will work fine in a notebook environment; in a vanilla python script you will need to import `asyncio` and wrap your code in an async function, like this:\n",
    "\n",
    "```\n",
    "async def main():\n",
    "    w = MyWorkflow(timeout=10, verbose=False)\n",
    "    result = await w.run()\n",
    "    print(result)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    asyncio.run(main())\n",
    "```\n",
    "\n",
    "Since we're in a notebook right now, we won't execute the above code as it won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbbee35",
   "metadata": {},
   "source": [
    "## Visualizing a workflow\n",
    "\n",
    "A great feature of workflows is the built-in visualizer, which we will install now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a110dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff163ad7",
   "metadata": {},
   "source": [
    "## Custom Events\n",
    "\n",
    "Multiple steps are created by defining custom events that can be emitted by steps and trigger other steps. Let's define a simple 3-step workflow by defining two custom events, `FirstEvent` and `SecondEvent`. These classes can have any names and properties, but must inherit from `Event`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class FirstEvent(Event):\n",
    "    first_output: str\n",
    "\n",
    "class SecondEvent(Event):\n",
    "    second_output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9665d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> FirstEvent:\n",
    "        print(ev.first_input)\n",
    "        return FirstEvent(first_output=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: FirstEvent) -> SecondEvent:\n",
    "        print(ev.first_output)\n",
    "        return SecondEvent(second_output=\"Second step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
    "        print(ev.second_output)\n",
    "        return StopEvent(result=\"Workflow complete.\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run(first_input=\"Start the workflow.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47aaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f4767",
   "metadata": {},
   "source": [
    "## Looping\n",
    "\n",
    "However, there's not much point to a workflow if it just runs straight through! A key feature of Workflows is their enablement of branching and looping logic, more simply and flexibly than graph-based approaches. To enable looping, we'll create a new `LoopEvent` (LoopEvent is not special, any event can be used to loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06290ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopEvent(Event):\n",
    "    first_input: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45eee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "\n",
    "    # step_one will trigger on a StartEvent or a LoopEvent\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> FirstEvent:\n",
    "        print(ev.first_input)\n",
    "        return FirstEvent(first_output=\"First step complete\")\n",
    "\n",
    "    # step two returns either a SecondEvent or a LoopEvent\n",
    "    @step\n",
    "    async def step_two(self, ev: FirstEvent) -> SecondEvent | LoopEvent:\n",
    "        print(ev.first_output)\n",
    "        if random.randint(0, 5) > 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(first_input=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return SecondEvent(second_output=\"Second step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ev: SecondEvent) -> StopEvent:\n",
    "        print(ev.second_output)\n",
    "        return StopEvent(result=\"Workflow complete.\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run(first_input=\"Start the workflow.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29782bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(MyWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d771c",
   "metadata": {},
   "source": [
    "## Branching\n",
    "\n",
    "The same constructs that allow us to loop allow us to create branches. Here's a workflow that executes two different branches depending on an early decision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7905e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BranchA1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchA2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB1Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchB2Event(Event):\n",
    "    payload: str\n",
    "\n",
    "\n",
    "class BranchWorkflow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ev: StartEvent) -> BranchA1Event | BranchB1Event:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Go to branch A\")\n",
    "            return BranchA1Event(payload=\"Branch A\")\n",
    "        else:\n",
    "            print(\"Go to branch B\")\n",
    "            return BranchB1Event(payload=\"Branch B\")\n",
    "\n",
    "    @step\n",
    "    async def step_a1(self, ev: BranchA1Event) -> BranchA2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchA2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_b1(self, ev: BranchB1Event) -> BranchB2Event:\n",
    "        print(ev.payload)\n",
    "        return BranchB2Event(payload=ev.payload)\n",
    "\n",
    "    @step\n",
    "    async def step_a2(self, ev: BranchA2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch A complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_b2(self, ev: BranchB2Event) -> StopEvent:\n",
    "        print(ev.payload)\n",
    "        return StopEvent(result=\"Branch B complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942bcf2c",
   "metadata": {},
   "source": [
    "## Concurrent execution\n",
    "\n",
    "The final form of flow control you can implement in workflows is concurrent execution. This allows you to efficiently run long-running tasks in parallel, and gather them together when they are needed. This can also let you perform map-reduce style tasks.\n",
    "\n",
    "To do this, we'll be using the `Context` object that we already encountered when working with `AgentWorkflow`. The `Context` is available to every step in a workflow: to access it, declare it as an argument to your step and it will be automatically populated.\n",
    "\n",
    "In this example, we use a new method, `Context.send_event` rather than returning an event. This allows us to emit multiple events in parallel rather than returning just one as we have previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9109e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class StepTwoEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class ParallelFlow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\n",
    "\n",
    "    @step(num_workers=4)\n",
    "    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StopEvent:\n",
    "        print(\"Running slow query \", ev.query)\n",
    "        await asyncio.sleep(random.randint(1, 5))\n",
    "\n",
    "        return StopEvent(result=ev.query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ParallelFlow(timeout=10, verbose=False)\n",
    "result = await w.run(message=\"Start the workflow.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c519c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(BranchWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158b799",
   "metadata": {},
   "source": [
    "As you can see, it executes all 3 queries. The first one to complete emits a StopEvent, at which point the workflow halts without waiting for the other 2 events.\n",
    "\n",
    "## Collecting events\n",
    "\n",
    "But what if we do want the output of all 3 events? Another method, `Context.collect_events`, exists for that purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe07aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepThreeEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class ConcurrentFlow(Workflow):\n",
    "    @step\n",
    "    async def start(self, ctx: Context, ev: StartEvent) -> StepTwoEvent:\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 1\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 2\"))\n",
    "        ctx.send_event(StepTwoEvent(query=\"Query 3\"))\n",
    "\n",
    "    @step(num_workers=4)\n",
    "    async def step_two(self, ctx: Context, ev: StepTwoEvent) -> StepThreeEvent:\n",
    "        print(\"Running query \", ev.query)\n",
    "        await asyncio.sleep(random.randint(1, 5))\n",
    "        return StepThreeEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ctx: Context, ev: StepThreeEvent) -> StopEvent:\n",
    "        # wait until we receive 3 events\n",
    "        result = ctx.collect_events(ev, [StepThreeEvent] * 3)\n",
    "        if result is None:\n",
    "            print(\"Not all events received yet.\")\n",
    "            return None\n",
    "\n",
    "        # do something with all 3 results together\n",
    "        print(result)\n",
    "        return StopEvent(result=\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e812743",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ConcurrentFlow(timeout=10, verbose=False)\n",
    "result = await w.run(message=\"Start the workflow.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0063142",
   "metadata": {},
   "source": [
    "In the above example, we emit three `StepTwoEvent`s which trigger `step_two`, each execution of which eventually emits a `StepThreeEvent`.\n",
    "\n",
    "`step_three` gets triggered whenever a `StepThreeEvent` fires. What `collect_events` does is store the events in the context until it has collected the number and type of events specified in its second argument. In this case, we've told it to wait for 3 events.\n",
    "\n",
    "If an event fires and `collect_events` hasn't yet seen the right number of events, it returns `None`, so we tell `step_three` to do nothing in that case. When `collect_events` receives the right number of events it returns them as an array, which you can see us printing in the final output. Note that in the array they are stored in the order they returned, not the order they were emitted.\n",
    "\n",
    "To implement a map-reduce pattern, you would split your task up into as many steps as necessary, and use `Context` to store that number with `ctx.set(\"num_events\", some_number)`. Then in `step_three` you would wait for the number stored in the context using `await ctx.get(\"num_events\")`. So you don't need to know in advance exactly how many concurrent steps you're taking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f335e",
   "metadata": {},
   "source": [
    "## Collecting different event types\n",
    "\n",
    "We don't just have to wait for multiple events of the same kind. In this example, we'll emit 3 totally different events and collect them at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepAEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class StepACompleteEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class StepBEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class StepBCompleteEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class StepCEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class StepCCompleteEvent(Event):\n",
    "    result: str\n",
    "\n",
    "class ConcurrentFlow(Workflow):\n",
    "    @step\n",
    "    async def start(\n",
    "        self, ctx: Context, ev: StartEvent\n",
    "    ) -> StepAEvent | StepBEvent | StepCEvent:\n",
    "        ctx.send_event(StepAEvent(query=\"Query 1\"))\n",
    "        ctx.send_event(StepBEvent(query=\"Query 2\"))\n",
    "        ctx.send_event(StepCEvent(query=\"Query 3\"))\n",
    "\n",
    "    @step\n",
    "    async def step_a(self, ctx: Context, ev: StepAEvent) -> StepACompleteEvent:\n",
    "        print(\"Doing something A-ish\")\n",
    "        return StepACompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_b(self, ctx: Context, ev: StepBEvent) -> StepBCompleteEvent:\n",
    "        print(\"Doing something B-ish\")\n",
    "        return StepBCompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_c(self, ctx: Context, ev: StepCEvent) -> StepCCompleteEvent:\n",
    "        print(\"Doing something C-ish\")\n",
    "        return StepCCompleteEvent(result=ev.query)\n",
    "\n",
    "    @step\n",
    "    async def step_three(\n",
    "        self,\n",
    "        ctx: Context,\n",
    "        ev: StepACompleteEvent | StepBCompleteEvent | StepCCompleteEvent,\n",
    "    ) -> StopEvent:\n",
    "        print(\"Received event \", ev.result)\n",
    "\n",
    "        # wait until we receive 3 events\n",
    "        events = ctx.collect_events(\n",
    "            ev,\n",
    "            [StepCCompleteEvent, StepACompleteEvent, StepBCompleteEvent],\n",
    "        )\n",
    "        if (events is None):\n",
    "            return None\n",
    "\n",
    "        # do something with all 3 results together\n",
    "        print(\"All events received: \", events)\n",
    "        return StopEvent(result=\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ConcurrentFlow(timeout=10, verbose=False)\n",
    "result = await w.run(message=\"Start the workflow.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(ConcurrentFlow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93f628",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "\n",
    "We already saw `AgentWorkflow` emit a stream of events that we could handle and filter when we run the event. Let's see how those get emitted in this example, using `Context.write_event_to_stream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4192b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstEvent(Event):\n",
    "    first_output: str\n",
    "\n",
    "class SecondEvent(Event):\n",
    "    second_output: str\n",
    "    response: str\n",
    "\n",
    "class TextEvent(Event):\n",
    "    delta: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ctx: Context, ev: StartEvent) -> FirstEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step one is happening\"))\n",
    "        return FirstEvent(first_output=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ctx: Context, ev: FirstEvent) -> SecondEvent:\n",
    "        llm = OpenAI(model=\"gpt-4.1-mini\", api_key=OPENAI_API_KEY) # the OpenAI key we set up at the beginning\n",
    "        generator = await llm.astream_complete(\n",
    "            \"Please give me the first 50 words of Moby Dick, a book in the public domain.\"\n",
    "        )\n",
    "        async for response in generator:\n",
    "            # Allow the workflow to stream this piece of response\n",
    "            ctx.write_event_to_stream(TextEvent(delta=response.delta))\n",
    "        return SecondEvent(\n",
    "            second_output=\"Second step complete, full response attached\",\n",
    "            response=str(response),\n",
    "        )\n",
    "\n",
    "    @step\n",
    "    async def step_three(self, ctx: Context, ev: SecondEvent) -> StopEvent:\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Step three is happening\"))\n",
    "        return StopEvent(result=\"Workflow complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = MyWorkflow(timeout=30, verbose=False)\n",
    "handler = workflow.run(first_input=\"Start the workflow.\")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ProgressEvent):\n",
    "        print(\"\\n\"+ev.msg)\n",
    "    if isinstance(ev, TextEvent):\n",
    "        print(ev.delta, end=\"\")\n",
    "\n",
    "final_result = await handler\n",
    "print(\"Final result = \", final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2860ce",
   "metadata": {},
   "source": [
    "# Putting together a multi-agent system\n",
    "\n",
    "Now we've seen all the ways workflows can work, let's construct our own multi-agent system! This will be our Deep Research multi-agent system.\n",
    "\n",
    "It will have three agents:\n",
    "* `QuestionAgent` which accepts a research topic and generates a bunch of questions\n",
    "* `AnswerAgent` which answers a specific question (we'll need to call it many times)\n",
    "* `ReportAgent` which aggregates all the answers and generates a report.\n",
    "\n",
    "We'll create these as `FunctionAgent`s, the same as we did when creating agents for `AgentWorkflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2f8d84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FunctionAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m question_agent = \u001b[43mFunctionAgent\u001b[49m(\n\u001b[32m      2\u001b[39m     tools=[],\n\u001b[32m      3\u001b[39m     llm=llm,\n\u001b[32m      4\u001b[39m     verbose=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      5\u001b[39m     system_prompt=\u001b[33m\"\"\"\u001b[39m\u001b[33mYou are part of a deep research system.\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m      Given a research topic, you should come up with a bunch of questions\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m      that a separate agent will answer in order to write a comprehensive\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m      report on that topic. To make it easy to answer the questions separately,\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m      you should provide the questions one per line. Don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt include markdown\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[33m      or any preamble in your response, just a list of questions.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m answer_agent = FunctionAgent(\n\u001b[32m     14\u001b[39m     tools=[search_local_corpus ],\n\u001b[32m     15\u001b[39m     llm=llm,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33m      on the topic, as many times as you need.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m report_agent = FunctionAgent(\n\u001b[32m     25\u001b[39m     tools=[],\n\u001b[32m     26\u001b[39m     llm=llm,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m \u001b[33m      them all into a comprehensive report on the topic.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     31\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'FunctionAgent' is not defined"
     ]
    }
   ],
   "source": [
    "question_agent = FunctionAgent(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    system_prompt=\"\"\"You are part of a deep research system.\n",
    "      Given a research topic, you should come up with a bunch of questions\n",
    "      that a separate agent will answer in order to write a comprehensive\n",
    "      report on that topic. To make it easy to answer the questions separately,\n",
    "      you should provide the questions one per line. Don't include markdown\n",
    "      or any preamble in your response, just a list of questions.\"\"\"\n",
    ")\n",
    "\n",
    "answer_agent = FunctionAgent(\n",
    "    tools=[search_local_corpus ],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    system_prompt=\"\"\"You are part of a deep research system.\n",
    "      Given a specific question, your job is to come up with a deep answer\n",
    "      to that question, which will be combined with other answers on the topic\n",
    "      into a comprehensive report. You can search the web to get information\n",
    "      on the topic, as many times as you need.\"\"\"\n",
    ")\n",
    "\n",
    "report_agent = FunctionAgent(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    system_prompt=\"\"\"You are part of a deep research system.\n",
    "      Given a set of answers to a set of questions, your job is to combine\n",
    "      them all into a comprehensive report on the topic.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6776f756",
   "metadata": {},
   "source": [
    "The Workflow we'll need to handle this task needs to do a few things:\n",
    "* Accept the topic and pass it to the QuestionAgent\n",
    "* Take all the answers from the QuestionAgent and split them up, firing off one AnswerAgent for each question\n",
    "* Aggregate all the questions and answers from the AnswerAgents\n",
    "* Generate a single report from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c3ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateEvent(Event):\n",
    "    research_topic: str\n",
    "\n",
    "class QuestionEvent(Event):\n",
    "    question: str\n",
    "\n",
    "class AnswerEvent(Event):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class ProgressEvent(Event):\n",
    "    msg: str\n",
    "\n",
    "class DeepResearchWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> GenerateEvent:\n",
    "        self.question_agent = ev.question_agent\n",
    "        self.answer_agent = ev.answer_agent\n",
    "        self.report_agent = ev.report_agent\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Starting research\"))\n",
    "\n",
    "        return GenerateEvent(research_topic=ev.research_topic)\n",
    "\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateEvent) -> QuestionEvent:\n",
    "\n",
    "        await ctx.set(\"research_topic\", ev.research_topic)\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=f\"Research topic is {ev.research_topic}\"))\n",
    "\n",
    "        result = await self.question_agent.run(user_msg=f\"\"\"Generate some questions\n",
    "          on the topic <topic>{ev.research_topic}</topic>.\"\"\")\n",
    "\n",
    "        # some basic string manipulation to get separate questions\n",
    "        lines = str(result).split(\"\\n\")\n",
    "        questions = [line.strip() for line in lines if line.strip() != \"\"]\n",
    "\n",
    "        # record how many answers we're going to need to wait for\n",
    "        await ctx.set(\"total_questions\", len(questions))\n",
    "\n",
    "        # fire off multiple Answer Agents\n",
    "        for question in questions:\n",
    "            ctx.send_event(QuestionEvent(question=question))\n",
    "\n",
    "    @step\n",
    "    async def answer_question(self, ctx: Context, ev: QuestionEvent) -> AnswerEvent:\n",
    "\n",
    "        result = await self.answer_agent.run(user_msg=f\"\"\"Research the answer to this\n",
    "          question: <question>{ev.question}</question>. You can use web\n",
    "          search to help you find information on the topic, as many times\n",
    "          as you need. Return just the answer without preamble or markdown.\"\"\")\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=f\"\"\"Received question {ev.question}\n",
    "            Came up with answer: {str(result)}\"\"\"))\n",
    "\n",
    "        return AnswerEvent(question=ev.question,answer=str(result))\n",
    "\n",
    "    @step\n",
    "    async def write_report(self, ctx: Context, ev: AnswerEvent) -> StopEvent:\n",
    "\n",
    "        research = ctx.collect_events(ev, [AnswerEvent] * await ctx.get(\"total_questions\"))\n",
    "        # if we haven't received all the answers yet, this will be None\n",
    "        if research is None:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=\"Collecting answers...\"))\n",
    "            return None\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Generating report...\"))\n",
    "\n",
    "        # aggregate the questions and answers\n",
    "        all_answers = \"\"\n",
    "        for q_and_a in research:\n",
    "            all_answers += f\"Question: {q_and_a.question}\\nAnswer: {q_and_a.answer}\\n\\n\"\n",
    "\n",
    "        # prompt the report\n",
    "        result = await self.report_agent.run(user_msg=f\"\"\"You are part of a deep research system.\n",
    "          You have been given a complex topic on which to write a report:\n",
    "          <topic>{await ctx.get(\"research_topic\")}.\n",
    "\n",
    "          Other agents have already come up with a list of questions about the\n",
    "          topic and answers to those questions. Your job is to write a clear,\n",
    "          thorough report that combines all the information from those answers.\n",
    "\n",
    "          Here are the questions and answers:\n",
    "          <questions_and_answers>{all_answers}</questions_and_answers>\"\"\")\n",
    "\n",
    "        return StopEvent(result=str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51eb937",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = DeepResearchWorkflow(timeout=600)\n",
    "handler = workflow.run(\n",
    "    research_topic=\"History of San Francisco\",\n",
    "    question_agent=question_agent,\n",
    "    answer_agent=answer_agent,\n",
    "    report_agent=report_agent\n",
    ")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ProgressEvent):\n",
    "        print(ev.msg)\n",
    "\n",
    "final_result = await handler\n",
    "print(\"==== The report ====\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19278d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e95e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(DeepResearchWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0921c",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "We now have a deep research agent! It comprehensively researches a topic for us before providing a detailed answer. But we can do better!\n",
    "\n",
    "LLMs are capable of self-reflection: they can read their own work, critique it, and provide feedback, allowing them to take a second try when they fall short.\n",
    "\n",
    "Let's add reflection to our deep research agent! This will involve several changes:\n",
    "\n",
    "* In `research` we'll store the research into the context, since we might need to use it multiple times\n",
    "* We'll tell `write` that it can be triggered by a `RewriteEvent` in addition to a `WriteEvent`\n",
    "* If it's a `RewriteEvent` we'll add the review as feedback to the prompt\n",
    "* `review` will be changed to optionally emit a `RewriteEvent`\n",
    "* We'll get the LLM to decide if the review returned by the agent is a \"bad\" or \"good\" review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dccf862",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackEvent(Event):\n",
    "    feedback: str\n",
    "\n",
    "class ReviewEvent(Event):\n",
    "    report: str\n",
    "\n",
    "review_agent = FunctionAgent(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    system_prompt=\"\"\"You are part of a deep research system.\n",
    "      Your job is to review a report that's been written and suggest\n",
    "      questions that could have been asked to produce a more comprehensive\n",
    "      report than the current version, or to decide that the current\n",
    "      report is comprehensive enough.\"\"\"\n",
    ")\n",
    "\n",
    "class DeepResearchWithReflectionWorkflow(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> GenerateEvent:\n",
    "        self.question_agent = ev.question_agent\n",
    "        self.answer_agent = ev.answer_agent\n",
    "        self.report_agent = ev.report_agent\n",
    "        self.review_agent = ev.review_agent\n",
    "        self.review_cycles = 0\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Starting research\"))\n",
    "\n",
    "        return GenerateEvent(research_topic=ev.research_topic)\n",
    "\n",
    "    @step\n",
    "    async def generate_questions(self, ctx: Context, ev: GenerateEvent | FeedbackEvent) -> QuestionEvent:\n",
    "\n",
    "        await ctx.set(\"research_topic\", ev.research_topic)\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=f\"Research topic is {ev.research_topic}\"))\n",
    "\n",
    "        prompt = f\"\"\"Generate some questions on the topic <topic>{ev.research_topic}</topic>.\"\"\"\n",
    "\n",
    "        if isinstance(ev, FeedbackEvent):\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=f\"Got feedback: {ev.feedback}\"))\n",
    "            prompt += f\"\"\"You have previously researched this topic and\n",
    "                got the following feedback, consisting of additional questions\n",
    "                you might want to ask: <feedback>{ev.feedback}</feedback>.\n",
    "                Keep this in mind when formulating your questions.\"\"\"\n",
    "\n",
    "        result = await self.question_agent.run(user_msg=prompt)\n",
    "\n",
    "        # some basic string manipulation to get separate questions\n",
    "        lines = str(result).split(\"\\n\")\n",
    "        questions = [line.strip() for line in lines if line.strip() != \"\"]\n",
    "\n",
    "        # record how many answers we're going to need to wait for\n",
    "        await ctx.set(\"total_questions\", len(questions))\n",
    "\n",
    "        # fire off multiple Answer Agents\n",
    "        for question in questions:\n",
    "            ctx.send_event(QuestionEvent(question=question))\n",
    "\n",
    "    @step\n",
    "    async def answer_question(self, ctx: Context, ev: QuestionEvent) -> AnswerEvent:\n",
    "\n",
    "        result = await self.answer_agent.run(user_msg=f\"\"\"Research the answer to this\n",
    "          question: <question>{ev.question}</question>. You can use web\n",
    "          search to help you find information on the topic, as many times\n",
    "          as you need. Return just the answer without preamble or markdown.\"\"\")\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=f\"\"\"Received question {ev.question}\n",
    "            Came up with answer: {str(result)}\"\"\"))\n",
    "\n",
    "        return AnswerEvent(question=ev.question,answer=str(result))\n",
    "\n",
    "    @step\n",
    "    async def write_report(self, ctx: Context, ev: AnswerEvent) -> ReviewEvent:\n",
    "\n",
    "        research = ctx.collect_events(ev, [AnswerEvent] * await ctx.get(\"total_questions\"))\n",
    "        # if we haven't received all the answers yet, this will be None\n",
    "        if research is None:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=\"Collecting answers...\"))\n",
    "            return None\n",
    "\n",
    "        ctx.write_event_to_stream(ProgressEvent(msg=\"Generating report...\"))\n",
    "\n",
    "        # aggregate the questions and answers\n",
    "        all_answers = \"\"\n",
    "        for q_and_a in research:\n",
    "            all_answers += f\"Question: {q_and_a.question}\\nAnswer: {q_and_a.answer}\\n\\n\"\n",
    "\n",
    "        # prompt the report\n",
    "        result = await self.report_agent.run(user_msg=f\"\"\"You are part of a deep research system.\n",
    "          You have been given a complex topic on which to write a report:\n",
    "          <topic>{await ctx.get(\"research_topic\")}.\n",
    "\n",
    "          Other agents have already come up with a list of questions about the\n",
    "          topic and answers to those questions. Your job is to write a clear,\n",
    "          thorough report that combines all the information from those answers.\n",
    "\n",
    "          Here are the questions and answers:\n",
    "          <questions_and_answers>{all_answers}</questions_and_answers>\"\"\")\n",
    "\n",
    "        return ReviewEvent(report=str(result))\n",
    "\n",
    "    @step\n",
    "    async def review(self, ctx: Context, ev: ReviewEvent) -> StopEvent | FeedbackEvent:\n",
    "\n",
    "        result = await self.review_agent.run(user_msg=f\"\"\"You are part of a deep research system.\n",
    "          You have just written a report about the topic {await ctx.get(\"research_topic\")}.\n",
    "          Here is the report: <report>{ev.report}</report>\n",
    "          Decide whether this report is sufficiently comprehensive.\n",
    "          If it is, respond with just the string \"ACCEPTABLE\" and nothing else.\n",
    "          If it needs more research, suggest some additional questions that could\n",
    "          have been asked.\"\"\")\n",
    "\n",
    "        self.review_cycles += 1\n",
    "\n",
    "        # either it's okay or we've already gone through 3 cycles\n",
    "        if str(result) == \"ACCEPTABLE\" or self.review_cycles >= 3:\n",
    "            return StopEvent(result=ev.report)\n",
    "        else:\n",
    "            ctx.write_event_to_stream(ProgressEvent(msg=\"Sending feedback\"))\n",
    "            return FeedbackEvent(\n",
    "                research_topic=await ctx.get(\"research_topic\"),\n",
    "                feedback=str(result)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9a499f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(DeepResearchWithReflectionWorkflow, filename=\"basic_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dddd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = DeepResearchWithReflectionWorkflow(timeout=600)\n",
    "handler = workflow.run(\n",
    "    research_topic=\"\"\"\n",
    "    History of San Francisco.\n",
    "    \"\"\",\n",
    "    question_agent=question_agent,\n",
    "    answer_agent=answer_agent,\n",
    "    report_agent=report_agent,\n",
    "    review_agent=review_agent\n",
    ")\n",
    "\n",
    "async for ev in handler.stream_events():\n",
    "    if isinstance(ev, ProgressEvent):\n",
    "        print(ev.msg)\n",
    "\n",
    "final_result = await handler\n",
    "print(\"==== The report ====\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e297acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(final_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2cad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c948daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-session",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
